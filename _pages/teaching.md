---
layout: page
permalink: /teaching/
title: Teaching
nav: true
nav_order: 5
---
<style>
  /* Global container setup */
  .content-area {
    display: flex; /* Enables flexbox layout */
    max-width: 1500px;
    justify-content: space-between; /* Distributes space between sections */
    align-items: flex-start; /* Aligns items at the top of their containers */
    flex-wrap: wrap; /* Allows items to wrap on smaller screens */
  }

  /* Section specific styles */
  .section {
    flex: 1; /* Each section takes equal space */
    margin: 30px; /* Margin between sections */
    margin-top: 10px; /* Margin between sections */
    min-width: 300px; 
    max-width: 325px; 
    box-shadow: 0 0 0px rgba(0,0,0,0.1); /* Optional: adds shadow for depth */
  }
  
  
  
    .intro-text {
    font-size: 0.9em; /* Adjust the font size of the introductory text */
    line-height:  auto; /* Increase line height for better readability */
    margin-bottom: 20px; /* Add some space after the paragraph */
  }
  

  /* Typography and readability improvements */
  li {
    font-size: 0.86em;
    line-height: 2.1em;
  }
  
  nav{
      font-size: 1.1em;
  }
  
  ul {
   line-height: 1.6 px;
    padding-left: 25px;
    list-style: number;
  }
    
  
    h1 {
    font-size: 1.8em;
    text-align: center;
    padding-top:5px;
  }
  
  h3 {
    font-size: 1.0em;
    padding: 10px;
        text-align: center;
  }

</style>

<div class="content-area">

<p class="intro-text">
Welcome to my teaching webpage! During my Ph.D. at the University of Cambridge, I participated in teaching the following courses:
</p>


  <!-- Statistics IB Section -->
  <div class="section">
    <h3><a href="https://www.statslab.cam.ac.uk/Dept/People/djsteaching/teaching17.html">Statistics IB</a></h3>
    <ul>
      <li>Introduction and probability revision </li>
      <li>Estimation, bias and mean squared error</li>
      <li>Sufficiency</li>
      <li>Maximum Likelihood Estimator (MLE)</li>
      <li>Confidence Intervals</li>
      <li>Bayesian estimation</li>
      <li>Simple Hypotheses</li>
      <li>Composite hypotheses</li>
      <li>Tests of goodness-of-fit and independence</li>
      <li>Tests in contingency tables</li>
      <li>Multivariate normal theory</li>
      <li>The linear model</li>
       <li>The normal linear model</li>
      <li>Inference in the normal linear model</li>
      <li>Special cases of the linear model</li>
      <li>Hypothesis testing in the linear model</li>
    </ul>
  </div>

  <!-- Principle of Statistics Section -->
  <div class="section">
    <h3><a href="https://q-berthet.github.io/notes/princip_stat_complete.pdf">Principle of Statistics</a></h3>
    <ul>
     <li>Course overview</li>
      <li>Fisher information</li>
      <li>Cramer-Rao bound</li>
      <li>Stochastic convergence</li>
      <li>Central limit theorem</li>
      <li>Consistency of the MLE</li>
      <li>Asymptotic normality of MLE</li>
       <li>Plug-in MLE and Delta method</li>
      <li>Asymptotic inference with MLE</li>
      <li>Introduction to Bayesian statistics</li>
       <li>Between prior and posterior</li>
      <li>Frequentist analysis of Bayesian methods</li>
     <li>Decision theory & Bayesian risk</li>
      <li>Minimax risk and admissibility</li>
      <li>Admissibility in the Gaussian model</li>
      <li>Risk of the James–Stein estimator</li>
      <li>Classification problems</li>
      <li>Multivariate analysis</li> 
      <li>Principal component analysis</li>
      <li>Resampling principles & the bootstrap</li>
      <li>Validity of the bootstrap</li>
      <li>Monte Carlo methods</li>
      <li>Markov chain Monte Carlo methods</li>
      <li>Introduction to Nonparametric statistics</li>
    </ul>
  </div>

  <!-- Mathematics of Machine Learning Section -->
  <div class="section">
    <h3><a href="https://www.statslab.cam.ac.uk/~rds37/machine_learning.html">Mathematics of Machine Learning</a></h3>
    <ul>
    <li>Review of conditional expectation</li>
      <li>Empirical risk minimisation</li>
      <li>Sub-Gaussianity and Hoeffding’s inequality</li>
      <li>Finite hypothesis classes</li>
      <li>Bounded difference inequality</li>
      <li>Rademacher complexity</li>
      <li>VC dimension</li>
      <li>Convex analysis</li>
      <li>Convex surrogates</li>
      <li>Rademacher complexity revisited</li>
       <li>Gradient descent</li>
      <li>Stochastic gradient descent</li>
      <li>Cross-validation</li>
      <li>Adaboost & Gradient boosting</li>
      <li>Decision trees & Random forests</li>
      <li>Feedforward neural networks</li>
    </ul>
  </div>
</div>
